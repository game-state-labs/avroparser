{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# FTC Contextualized Events - Duplicate Analysis\n",
    "\n",
    "This notebook analyzes duplicate events in `out/ftc-contextualized-events.csv` to identify duplication patterns.\n",
    "\n",
    "## Duplicate Detection Strategies\n",
    "\n",
    "1. **Exact Duplicates (by event_id)**: Same `event_id` appearing multiple times - PRIMARY FOCUS\n",
    "2. **Duplicates by event_id + group_id**: Same event_id within/across groups - SECONDARY FOCUS\n",
    "3. **Duplicates by event_id + player_id**: Same event_id for same player\n",
    "4. **Duplicates by event_id + session_id**: Same event_id within same session\n",
    "5. **Content Duplicates**: Same `player_id` + `event_name` + `event_timestamp` + `payload`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:05:51.045385Z",
     "iopub.status.busy": "2026-01-26T07:05:51.045308Z",
     "iopub.status.idle": "2026-01-26T07:06:34.222159Z",
     "shell.execute_reply": "2026-01-26T07:06:34.221516Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('out/ftc-contextualized-events.csv', low_memory=False)\n",
    "\n",
    "# Explicitly convert timestamp column to datetime\n",
    "# Use ISO8601 format to handle both with and without microseconds\n",
    "df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], format='ISO8601', utc=True)\n",
    "\n",
    "print(f\"Loaded {len(df):,} events\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview-header",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overview",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:34.225282Z",
     "iopub.status.busy": "2026-01-26T07:06:34.225103Z",
     "iopub.status.idle": "2026-01-26T07:06:35.823821Z",
     "shell.execute_reply": "2026-01-26T07:06:35.823145Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal events: {len(df):,}\")\n",
    "print(f\"Unique event_ids: {df['event_id'].nunique():,}\")\n",
    "print(f\"Unique group_ids: {df['group_id'].nunique():,}\")\n",
    "print(f\"Unique players: {df['player_id'].nunique():,}\")\n",
    "print(f\"Unique sessions: {df['session_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nDate range: {df['event_timestamp'].min()} to {df['event_timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Event Types Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "event_counts = df['event_name'].value_counts()\n",
    "for event_name, count in event_counts.items():\n",
    "    print(f\"  {event_name}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize event type distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "event_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_title('Event Type Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Event Type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-dups-header",
   "metadata": {},
   "source": [
    "## 3. Exact Duplicates (by event_id) - PRIMARY FOCUS\n",
    "\n",
    "Identifying events where the same `event_id` appears multiple times. This is the most basic and important form of duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:35.825279Z",
     "iopub.status.busy": "2026-01-26T07:06:35.825180Z",
     "iopub.status.idle": "2026-01-26T07:06:38.080476Z",
     "shell.execute_reply": "2026-01-26T07:06:38.080051Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXACT DUPLICATES (by event_id) - PRIMARY FOCUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count occurrences of each event_id\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1]\n",
    "\n",
    "total_events = len(df)\n",
    "unique_event_ids = df['event_id'].nunique()\n",
    "duplicate_event_id_count = len(duplicated_event_ids)\n",
    "events_with_dup_ids = df[df['event_id'].isin(duplicated_event_ids.index)]\n",
    "total_duplicate_rows = len(events_with_dup_ids)\n",
    "\n",
    "print(f\"\\nTotal events: {total_events:,}\")\n",
    "print(f\"Unique event_ids: {unique_event_ids:,}\")\n",
    "print(f\"Duplicated event_ids: {duplicate_event_id_count:,}\")\n",
    "print(f\"Events with duplicated event_id: {total_duplicate_rows:,}\")\n",
    "print(f\"Duplication rate: {(total_events - unique_event_ids) / total_events * 100:.2f}%\")\n",
    "\n",
    "if duplicate_event_id_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample Duplicated event_ids (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for event_id, count in duplicated_event_ids.head(10).items():\n",
    "        print(f\"  {event_id}: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first duplicated event_id):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_id = duplicated_event_ids.index[0]\n",
    "    display(df[df['event_id'] == first_dup_id][['event_id', 'player_id', 'session_id', 'event_name', 'event_timestamp', 'group_id']])\n",
    "else:\n",
    "    print(\"\\nNo exact duplicates found by event_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group-dups-header",
   "metadata": {},
   "source": [
    "## 4. Duplicates by event_id + group_id - SECONDARY FOCUS\n",
    "\n",
    "Analyzing whether the same `event_id` appears in the same group or across different groups. This helps understand if duplicates are within-batch or cross-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:38.081610Z",
     "iopub.status.busy": "2026-01-26T07:06:38.081527Z",
     "iopub.status.idle": "2026-01-26T07:06:42.449728Z",
     "shell.execute_reply": "2026-01-26T07:06:42.449060Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + group_id - SECONDARY FOCUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# remove event_name = 'session_end'\n",
    "df = df[df['event_name'] != 'session_end']\n",
    "\n",
    "# Group by event_id + group_id\n",
    "df['event_group_key'] = df['event_id'] + '_' + df['group_id'].astype(str)\n",
    "event_group_counts = df['event_group_key'].value_counts()\n",
    "duplicated_event_group = event_group_counts[event_group_counts > 1]\n",
    "\n",
    "unique_event_group_combos = df['event_group_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_group)\n",
    "\n",
    "print(f\"\\nUnique event_id + group_id combinations: {unique_event_group_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_event_group_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Analyze within-group vs cross-group duplicates\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "\n",
    "if len(duplicated_event_ids) > 0:\n",
    "    dup_events = df[df['event_id'].isin(duplicated_event_ids)]\n",
    "    groups_per_event = dup_events.groupby('event_id')['group_id'].nunique()\n",
    "    \n",
    "    within_group = (groups_per_event == 1).sum()\n",
    "    cross_group = (groups_per_event > 1).sum()\n",
    "    \n",
    "    print(f\"\\n\" + \"-\" * 40)\n",
    "    print(\"Duplicate Distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total duplicated event_ids: {len(duplicated_event_ids):,}\")\n",
    "    print(f\"Within-group duplicates: {within_group:,} ({within_group/len(duplicated_event_ids)*100:.1f}%)\")\n",
    "    print(f\"Cross-group duplicates: {cross_group:,} ({cross_group/len(duplicated_event_ids)*100:.1f}%)\")\n",
    "    \n",
    "    if duplicate_combo_count > 0:\n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(\"Sample duplicated event_id + group_id (top 10):\")\n",
    "        print(\"-\" * 40)\n",
    "        for key, count in duplicated_event_group.head(10).items():\n",
    "            parts = key.rsplit('_', 1)\n",
    "            event_id = parts[0] if len(parts) > 1 else key[:36]\n",
    "            group_id = parts[1] if len(parts) > 1 else 'N/A'\n",
    "            print(f\"  event_id={event_id[:30]}..., group_id={group_id}: {count} occurrences\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    labels = ['Within-Group', 'Cross-Group']\n",
    "    sizes = [within_group, cross_group]\n",
    "    colors = ['steelblue', 'coral']\n",
    "    if sum(sizes) > 0:\n",
    "        ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax.set_title('Distribution of Duplicate Types (Within vs Cross Group)', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo duplicated event_ids found to analyze by group.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_group_key', axis=1, inplace=True)\n",
    "\n",
    "# Print Cross-group duplicates event_id, group_id, player_id, session_id and event_name\n",
    "print(\"\\nCross-group duplicate event details (event_id, group_id, player_id, session_id, event_name):\")\n",
    "if cross_group > 0:\n",
    "    cross_group_event_ids = groups_per_event[groups_per_event > 1].index\n",
    "    cross_dup_events = df[df['event_id'].isin(cross_group_event_ids)][['event_id', 'group_id', 'player_id', 'session_id', 'event_name', 'event_timestamp', 'payload']]\n",
    "    display(cross_dup_events.head(10))\n",
    "else:\n",
    "    print(\"No cross-group duplicate event_ids found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "player-dups-header",
   "metadata": {},
   "source": [
    "## 5. Duplicates by event_id + player_id\n",
    "\n",
    "Identifying the same `event_id` appearing for the same player multiple times. This detects re-processing of the same event for a specific player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "player-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:42.451413Z",
     "iopub.status.busy": "2026-01-26T07:06:42.451310Z",
     "iopub.status.idle": "2026-01-26T07:06:44.856766Z",
     "shell.execute_reply": "2026-01-26T07:06:44.856374Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + player_id\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by event_id + player_id\n",
    "df['event_player_key'] = df['event_id'] + '_' + df['player_id'].astype(str)\n",
    "event_player_counts = df['event_player_key'].value_counts()\n",
    "duplicated_event_player = event_player_counts[event_player_counts > 1]\n",
    "\n",
    "unique_event_player_combos = df['event_player_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_player)\n",
    "\n",
    "print(f\"\\nUnique event_id + player_id combinations: {unique_event_player_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_event_player_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_combo_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicated event_id + player_id (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_event_player.head(10).items():\n",
    "        parts = key.rsplit('_', 1)\n",
    "        event_id = parts[0] if len(parts) > 1 else key[:36]\n",
    "        player_id = parts[1] if len(parts) > 1 else 'N/A'\n",
    "        print(f\"  event_id={event_id[:30]}..., player_id={player_id}: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first combo):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_event_player.index[0]\n",
    "    display(df[df['event_player_key'] == first_dup_key][['event_id', 'player_id', 'session_id', 'event_name', 'event_timestamp', 'group_id']])\n",
    "else:\n",
    "    print(\"\\nNo duplicates found by event_id + player_id.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_player_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-dups-header",
   "metadata": {},
   "source": [
    "## 6. Duplicates by event_id + session_id\n",
    "\n",
    "Identifying the same `event_id` appearing within the same session multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:44.858114Z",
     "iopub.status.busy": "2026-01-26T07:06:44.858015Z",
     "iopub.status.idle": "2026-01-26T07:06:47.340057Z",
     "shell.execute_reply": "2026-01-26T07:06:47.339667Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + session_id\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by event_id + session_id\n",
    "df['event_session_key'] = df['event_id'] + '_' + df['session_id'].astype(str)\n",
    "event_session_counts = df['event_session_key'].value_counts()\n",
    "duplicated_event_session = event_session_counts[event_session_counts > 1]\n",
    "\n",
    "unique_event_session_combos = df['event_session_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_session)\n",
    "\n",
    "print(f\"\\nUnique event_id + session_id combinations: {unique_event_session_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_event_session_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_combo_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicated event_id + session_id (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_event_session.head(10).items():\n",
    "        parts = key.rsplit('_', 1)\n",
    "        event_id = parts[0] if len(parts) > 1 else key[:36]\n",
    "        session_id = parts[1] if len(parts) > 1 else 'N/A'\n",
    "        print(f\"  event_id={event_id[:30]}..., session_id={session_id[:20]}...: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first combo):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_event_session.index[0]\n",
    "    display(df[df['event_session_key'] == first_dup_key][['event_id', 'player_id', 'session_id', 'event_name', 'event_timestamp', 'group_id']])\n",
    "else:\n",
    "    print(\"\\nNo duplicates found by event_id + session_id.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_session_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-dups-header",
   "metadata": {},
   "source": [
    "## 7. Content-Based Duplicates\n",
    "\n",
    "Identifying events with the same `player_id` + `event_name` + `event_timestamp` + `payload`. These are events that are identical in content regardless of their `event_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:47.342159Z",
     "iopub.status.busy": "2026-01-26T07:06:47.342036Z",
     "iopub.status.idle": "2026-01-26T07:06:56.861370Z",
     "shell.execute_reply": "2026-01-26T07:06:56.860877Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CONTENT-BASED DUPLICATES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create content key\n",
    "df['payload_str'] = df['payload'].fillna('').astype(str)\n",
    "df['content_key'] = df['player_id'].astype(str) + '_' + df['event_name'] + '_' + df['event_timestamp'].astype(str) + '_' + df['payload_str']\n",
    "\n",
    "content_counts = df['content_key'].value_counts()\n",
    "duplicated_content = content_counts[content_counts > 1]\n",
    "\n",
    "unique_content = df['content_key'].nunique()\n",
    "duplicate_content_count = len(duplicated_content)\n",
    "events_with_dup_content = df[df['content_key'].isin(duplicated_content.index)]\n",
    "\n",
    "print(f\"\\nUnique content combinations: {unique_content:,}\")\n",
    "print(f\"Duplicated content combinations: {duplicate_content_count:,}\")\n",
    "print(f\"Events with duplicated content: {len(events_with_dup_content):,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_content) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_content_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample content duplicates (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_content.head(10).items():\n",
    "        print(f\"  {count} occurrences: {key[:80]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first content key):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_content.index[0]\n",
    "    display(df[df['content_key'] == first_dup_key][['event_id', 'player_id', 'event_name', 'event_timestamp', 'group_id']])\n",
    "else:\n",
    "    print(\"\\nNo content-based duplicates found.\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['payload_str', 'content_key'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "event-type-dups-header",
   "metadata": {},
   "source": [
    "## 8. Duplicate Trends by Event Type\n",
    "\n",
    "Analyzing which event types have the most duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "event-type-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:56.863467Z",
     "iopub.status.busy": "2026-01-26T07:06:56.863366Z",
     "iopub.status.idle": "2026-01-26T07:06:59.094090Z",
     "shell.execute_reply": "2026-01-26T07:06:59.092957Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE TRENDS BY EVENT TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicates by event_id\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "df['is_duplicate'] = df['event_id'].isin(duplicated_event_ids)\n",
    "\n",
    "# Analyze by event type\n",
    "event_type_analysis = df.groupby('event_name').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    duplicate_events=('is_duplicate', 'sum'),\n",
    "    unique_event_ids=('event_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "event_type_analysis['duplication_rate'] = (event_type_analysis['total_events'] - event_type_analysis['unique_event_ids']) / event_type_analysis['total_events'] * 100\n",
    "event_type_analysis = event_type_analysis.sort_values('duplicate_events', ascending=False)\n",
    "\n",
    "print(\"\\nDuplication by Event Type:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Event Type':<25} {'Total':>10} {'Duplicates':>12} {'Rate':>10}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in event_type_analysis.iterrows():\n",
    "    print(f\"{row['event_name']:<25} {row['total_events']:>10,} {int(row['duplicate_events']):>12,} {row['duplication_rate']:>9.2f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Duplicate count by event type\n",
    "ax1 = axes[0]\n",
    "event_type_analysis_sorted = event_type_analysis.sort_values('duplicate_events', ascending=True)\n",
    "ax1.barh(event_type_analysis_sorted['event_name'], event_type_analysis_sorted['duplicate_events'], color='coral', edgecolor='black')\n",
    "ax1.set_xlabel('Number of Duplicate Events')\n",
    "ax1.set_title('Duplicate Events by Type', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Duplication rate by event type\n",
    "ax2 = axes[1]\n",
    "event_type_analysis_rate = event_type_analysis.sort_values('duplication_rate', ascending=True)\n",
    "ax2.barh(event_type_analysis_rate['event_name'], event_type_analysis_rate['duplication_rate'], color='steelblue', edgecolor='black')\n",
    "ax2.set_xlabel('Duplication Rate (%)')\n",
    "ax2.set_title('Duplication Rate by Event Type', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "df.drop('is_duplicate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-dups-header",
   "metadata": {},
   "source": [
    "## 9. Duplicate Trends Over Time\n",
    "\n",
    "Analyzing how duplicates are distributed over time (hourly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-dups",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:06:59.098423Z",
     "iopub.status.busy": "2026-01-26T07:06:59.098251Z",
     "iopub.status.idle": "2026-01-26T07:07:01.595050Z",
     "shell.execute_reply": "2026-01-26T07:07:01.594576Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE TRENDS OVER TIME\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicates\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "df['is_duplicate'] = df['event_id'].isin(duplicated_event_ids)\n",
    "\n",
    "# Add time-based columns\n",
    "df['hour'] = df['event_timestamp'].dt.floor('h')\n",
    "df['date'] = df['event_timestamp'].dt.date\n",
    "\n",
    "# Hourly analysis\n",
    "hourly_analysis = df.groupby('hour').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    duplicate_events=('is_duplicate', 'sum'),\n",
    "    unique_event_ids=('event_id', 'nunique')\n",
    ").reset_index()\n",
    "hourly_analysis['duplication_rate'] = (hourly_analysis['total_events'] - hourly_analysis['unique_event_ids']) / hourly_analysis['total_events'] * 100\n",
    "\n",
    "print(f\"\\nTime range: {df['event_timestamp'].min()} to {df['event_timestamp'].max()}\")\n",
    "print(f\"Total hours covered: {hourly_analysis['hour'].nunique()}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Events over time\n",
    "ax1 = axes[0]\n",
    "ax1.fill_between(hourly_analysis['hour'], hourly_analysis['total_events'], alpha=0.3, label='Total Events', color='steelblue')\n",
    "ax1.plot(hourly_analysis['hour'], hourly_analysis['total_events'], color='steelblue', linewidth=2, label='Total Events')\n",
    "ax1.fill_between(hourly_analysis['hour'], hourly_analysis['duplicate_events'], alpha=0.5, label='Duplicate Events', color='coral')\n",
    "ax1.plot(hourly_analysis['hour'], hourly_analysis['duplicate_events'], color='coral', linewidth=2, label='Duplicate Events')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Number of Events')\n",
    "ax1.set_title('Events Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Duplication rate over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(hourly_analysis['hour'], hourly_analysis['duplication_rate'], color='darkred', linewidth=2, marker='o', markersize=3)\n",
    "ax2.fill_between(hourly_analysis['hour'], hourly_analysis['duplication_rate'], alpha=0.3, color='darkred')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Duplication Rate (%)')\n",
    "ax2.set_title('Duplication Rate Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "df.drop(['is_duplicate', 'hour', 'date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group-analysis-header",
   "metadata": {},
   "source": [
    "## 10. Duplicates by Group (Batch Analysis)\n",
    "\n",
    "Analyzing which groups have the most duplicates and understanding the distribution across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group-analysis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:07:01.596337Z",
     "iopub.status.busy": "2026-01-26T07:07:01.596250Z",
     "iopub.status.idle": "2026-01-26T07:07:04.419311Z",
     "shell.execute_reply": "2026-01-26T07:07:04.418931Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY GROUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group-level analysis\n",
    "group_analysis = df.groupby('group_id').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    unique_event_ids=('event_id', 'nunique')\n",
    ").reset_index()\n",
    "group_analysis['duplicates_in_group'] = group_analysis['total_events'] - group_analysis['unique_event_ids']\n",
    "group_analysis['duplication_rate'] = group_analysis['duplicates_in_group'] / group_analysis['total_events'] * 100\n",
    "group_analysis = group_analysis.sort_values('duplicates_in_group', ascending=False)\n",
    "\n",
    "groups_with_dups = group_analysis[group_analysis['duplicates_in_group'] > 0]\n",
    "print(f\"\\nTotal groups: {len(group_analysis):,}\")\n",
    "print(f\"Groups with duplicates: {len(groups_with_dups):,}\")\n",
    "\n",
    "if len(groups_with_dups) > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Groups with most duplicates (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for _, row in group_analysis.head(10).iterrows():\n",
    "        group_id_str = str(row['group_id'])[:30] if len(str(row['group_id'])) > 30 else str(row['group_id'])\n",
    "        print(f\"  Group {group_id_str}...: {int(row['duplicates_in_group']):,} duplicates ({row['duplication_rate']:.1f}%) of {int(row['total_events']):,} events\")\n",
    "    \n",
    "    # Visualization - Duplication rate distribution across groups\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Histogram of duplication rates\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(group_analysis['duplication_rate'], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Duplication Rate (%)')\n",
    "    ax1.set_ylabel('Number of Groups')\n",
    "    ax1.set_title('Distribution of Duplication Rates Across Groups', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Top groups by duplicate count\n",
    "    ax2 = axes[1]\n",
    "    top_groups = group_analysis.head(10).copy()\n",
    "    top_groups['group_label'] = top_groups['group_id'].astype(str).str[:15]\n",
    "    ax2.barh(top_groups['group_label'], top_groups['duplicates_in_group'], color='coral', edgecolor='black')\n",
    "    ax2.set_xlabel('Number of Duplicates')\n",
    "    ax2.set_title('Top 10 Groups by Duplicate Count', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo duplicates found within any group.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "Overall summary of duplication findings across all detection strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T07:07:04.421271Z",
     "iopub.status.busy": "2026-01-26T07:07:04.421152Z",
     "iopub.status.idle": "2026-01-26T07:07:15.728982Z",
     "shell.execute_reply": "2026-01-26T07:07:15.728600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate all metrics\n",
    "total_events = len(df)\n",
    "\n",
    "# Strategy 1: Exact duplicates by event_id\n",
    "unique_event_ids = df['event_id'].nunique()\n",
    "exact_dup_rate = (total_events - unique_event_ids) / total_events * 100\n",
    "\n",
    "# Strategy 2: Duplicates by event_id + group_id\n",
    "df['event_group_key'] = df['event_id'] + '_' + df['group_id'].astype(str)\n",
    "unique_event_group = df['event_group_key'].nunique()\n",
    "event_group_dup_rate = (total_events - unique_event_group) / total_events * 100\n",
    "df.drop('event_group_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 3: Duplicates by event_id + player_id\n",
    "df['event_player_key'] = df['event_id'] + '_' + df['player_id'].astype(str)\n",
    "unique_event_player = df['event_player_key'].nunique()\n",
    "event_player_dup_rate = (total_events - unique_event_player) / total_events * 100\n",
    "df.drop('event_player_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 4: Duplicates by event_id + session_id\n",
    "df['event_session_key'] = df['event_id'] + '_' + df['session_id'].astype(str)\n",
    "unique_event_session = df['event_session_key'].nunique()\n",
    "event_session_dup_rate = (total_events - unique_event_session) / total_events * 100\n",
    "df.drop('event_session_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 5: Content-based duplicates\n",
    "df['payload_str'] = df['payload'].fillna('').astype(str)\n",
    "df['content_key'] = df['player_id'].astype(str) + '_' + df['event_name'] + '_' + df['event_timestamp'].astype(str) + '_' + df['payload_str']\n",
    "unique_content = df['content_key'].nunique()\n",
    "content_dup_rate = (total_events - unique_content) / total_events * 100\n",
    "df.drop(['payload_str', 'content_key'], axis=1, inplace=True)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Detection Strategy': [\n",
    "        '1. Exact (by event_id) - PRIMARY',\n",
    "        '2. event_id + group_id - SECONDARY',\n",
    "        '3. event_id + player_id',\n",
    "        '4. event_id + session_id',\n",
    "        '5. Content-based (player+event+time+payload)'\n",
    "    ],\n",
    "    'Unique Count': [\n",
    "        unique_event_ids,\n",
    "        unique_event_group,\n",
    "        unique_event_player,\n",
    "        unique_event_session,\n",
    "        unique_content\n",
    "    ],\n",
    "    'Duplicate Count': [\n",
    "        total_events - unique_event_ids,\n",
    "        total_events - unique_event_group,\n",
    "        total_events - unique_event_player,\n",
    "        total_events - unique_event_session,\n",
    "        total_events - unique_content\n",
    "    ],\n",
    "    'Duplication Rate': [\n",
    "        f\"{exact_dup_rate:.2f}%\",\n",
    "        f\"{event_group_dup_rate:.2f}%\",\n",
    "        f\"{event_player_dup_rate:.2f}%\",\n",
    "        f\"{event_session_dup_rate:.2f}%\",\n",
    "        f\"{content_dup_rate:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"\\nTotal Events: {total_events:,}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Duplication Summary by Detection Strategy:\")\n",
    "print(\"-\" * 70)\n",
    "display(summary_df)\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if exact_dup_rate > 0:\n",
    "    print(f\"\\n- Found {exact_dup_rate:.2f}% exact duplicates by event_id\")\n",
    "else:\n",
    "    print(\"\\n- No exact duplicates found by event_id\")\n",
    "\n",
    "if event_group_dup_rate < exact_dup_rate:\n",
    "    print(f\"- event_id + group_id has lower rate ({event_group_dup_rate:.2f}%), indicating some duplicates span groups\")\n",
    "\n",
    "if content_dup_rate > exact_dup_rate:\n",
    "    print(f\"- Content-based analysis reveals {content_dup_rate:.2f}% duplicates (higher than exact ID check)\")\n",
    "    print(\"  This suggests some duplicate content has different event_ids\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "strategies = ['Exact\\n(event_id)\\nPRIMARY', 'event_id +\\ngroup_id\\nSECONDARY', 'event_id +\\nplayer_id', 'event_id +\\nsession_id', 'Content-\\nbased']\n",
    "rates = [exact_dup_rate, event_group_dup_rate, event_player_dup_rate, event_session_dup_rate, content_dup_rate]\n",
    "colors = ['coral', 'steelblue', 'mediumseagreen', 'orchid', 'gold']\n",
    "\n",
    "bars = ax.bar(strategies, rates, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Duplication Rate (%)')\n",
    "ax.set_title('Duplication Rate by Detection Strategy', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, rates):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{rate:.2f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
