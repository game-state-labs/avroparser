{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Duplicate Event Analysis\n",
    "\n",
    "This notebook analyzes duplicate events in the exported raw events CSV to identify duplication patterns and trends.\n",
    "\n",
    "## Duplicate Detection Strategies\n",
    "\n",
    "1. **Exact Duplicates (by event_id)**: Same `event_id` appearing multiple times\n",
    "2. **Duplicates by event_id + player_id**: Same event_id appearing for the same player\n",
    "3. **Duplicates by event_id + session_id + player_id**: Same event_id within same session and player\n",
    "4. **Content Duplicates**: Same `playerID` + `event_name` + `event_timestamp` + `payload`\n",
    "5. **Near-Duplicates**: Same content within a short time window (possible re-sends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('out/output.csv', low_memory=False)\n",
    "\n",
    "# Explicitly convert timestamp column to datetime\n",
    "# Use ISO8601 format to handle both with and without microseconds\n",
    "df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], format='ISO8601', utc=True)\n",
    "\n",
    "print(f\"Loaded {len(df):,} events\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview-header",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal events: {len(df):,}\")\n",
    "print(f\"Unique event_ids: {df['event_id'].nunique():,}\")\n",
    "print(f\"Unique players: {df['playerID'].nunique():,}\")\n",
    "print(f\"Unique sessions: {df['session_id'].nunique():,}\")\n",
    "print(f\"Unique batches: {df['batchID'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nDate range: {df['event_timestamp'].min()} to {df['event_timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Event Types Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "event_counts = df['event_name'].value_counts()\n",
    "for event_name, count in event_counts.items():\n",
    "    print(f\"  {event_name}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize event type distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "event_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_title('Event Type Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Event Type')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-dups-header",
   "metadata": {},
   "source": [
    "## 3. Exact Duplicates (by event_id)\n",
    "\n",
    "Identifying events where the same `event_id` appears multiple times. This is the most basic form of duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXACT DUPLICATES (by event_id)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count occurrences of each event_id\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1]\n",
    "\n",
    "total_events = len(df)\n",
    "unique_event_ids = df['event_id'].nunique()\n",
    "duplicate_event_id_count = len(duplicated_event_ids)\n",
    "events_with_dup_ids = df[df['event_id'].isin(duplicated_event_ids.index)]\n",
    "total_duplicate_rows = len(events_with_dup_ids)\n",
    "\n",
    "print(f\"\\nTotal events: {total_events:,}\")\n",
    "print(f\"Unique event_ids: {unique_event_ids:,}\")\n",
    "print(f\"Duplicated event_ids: {duplicate_event_id_count:,}\")\n",
    "print(f\"Events with duplicated event_id: {total_duplicate_rows:,}\")\n",
    "print(f\"Duplication rate: {(total_events - unique_event_ids) / total_events * 100:.2f}%\")\n",
    "\n",
    "if duplicate_event_id_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample Duplicated event_ids (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for event_id, count in duplicated_event_ids.head(10).items():\n",
    "        print(f\"  {event_id}: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first duplicated event_id):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_id = duplicated_event_ids.index[0]\n",
    "    display(df[df['event_id'] == first_dup_id][['event_id', 'playerID', 'session_id', 'event_name', 'event_timestamp', 'batchID']])\n",
    "else:\n",
    "    print(\"\\nNo exact duplicates found by event_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "player-dups-header",
   "metadata": {},
   "source": [
    "## 4. Duplicates by event_id + player_id\n",
    "\n",
    "Identifying the same `event_id` appearing for the same player multiple times. This detects re-processing of the same event for a specific player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "player-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + playerID\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by event_id + playerID\n",
    "df['event_player_key'] = df['event_id'] + '_' + df['playerID']\n",
    "event_player_counts = df['event_player_key'].value_counts()\n",
    "duplicated_event_player = event_player_counts[event_player_counts > 1]\n",
    "\n",
    "unique_event_player_combos = df['event_player_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_player)\n",
    "\n",
    "print(f\"\\nUnique event_id + playerID combinations: {unique_event_player_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_event_player_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_combo_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicated event_id + playerID (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_event_player.head(10).items():\n",
    "        event_id, player_id = key.rsplit('_', 1)\n",
    "        print(f\"  event_id={event_id[:30]}..., playerID={player_id}: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first combo):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_event_player.index[0]\n",
    "    display(df[df['event_player_key'] == first_dup_key][['event_id', 'playerID', 'session_id', 'event_name', 'event_timestamp', 'batchID']])\n",
    "else:\n",
    "    print(\"\\nNo duplicates found by event_id + playerID.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_player_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70hfji7hqt8",
   "metadata": {},
   "source": [
    "## 5. Duplicates by event_id + session_id\n",
    "\n",
    "Identifying the same `event_id` appearing within the same session multiple times. This detects session-level duplicate sends regardless of player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymc0mlem7dn",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + session_id\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by event_id + session_id\n",
    "df['event_session_key'] = df['event_id'] + '_' + df['session_id']\n",
    "event_session_counts = df['event_session_key'].value_counts()\n",
    "duplicated_event_session = event_session_counts[event_session_counts > 1]\n",
    "\n",
    "unique_event_session_combos = df['event_session_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_session)\n",
    "\n",
    "print(f\"\\nUnique event_id + session_id combinations: {unique_event_session_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_event_session_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_combo_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicated event_id + session_id (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_event_session.head(10).items():\n",
    "        parts = key.rsplit('_', 1)\n",
    "        event_id = parts[0] if len(parts) > 1 else key[:36]\n",
    "        session_id = parts[1] if len(parts) > 1 else key[37:]\n",
    "        print(f\"  event_id={event_id[:30]}..., session_id={session_id[:20]}...: {count} occurrences\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first combo):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_event_session.index[0]\n",
    "    display(df[df['event_session_key'] == first_dup_key][['event_id', 'playerID', 'session_id', 'event_name', 'event_timestamp', 'batchID']])\n",
    "else:\n",
    "    print(\"\\nNo duplicates found by event_id + session_id.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_session_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-dups-header",
   "metadata": {},
   "source": [
    "## 5. Duplicates by event_id + session_id + player_id\n",
    "\n",
    "Identifying the same `event_id` appearing within the same session and player. This detects within-session duplicate sends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY event_id + session_id + playerID\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by event_id + session_id + playerID\n",
    "df['event_session_player_key'] = df['event_id'] + '_' + df['session_id'] + '_' + df['playerID']\n",
    "event_session_player_counts = df['event_session_player_key'].value_counts()\n",
    "duplicated_event_session_player = event_session_player_counts[event_session_player_counts > 1]\n",
    "\n",
    "unique_combos = df['event_session_player_key'].nunique()\n",
    "duplicate_combo_count = len(duplicated_event_session_player)\n",
    "\n",
    "print(f\"\\nUnique event_id + session_id + playerID combinations: {unique_combos:,}\")\n",
    "print(f\"Duplicated combinations: {duplicate_combo_count:,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_combos) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_combo_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicated combinations (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_event_session_player.head(10).items():\n",
    "        print(f\"  Key: {key[:60]}... ({count} occurrences)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first combo):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_event_session_player.index[0]\n",
    "    display(df[df['event_session_player_key'] == first_dup_key][['event_id', 'playerID', 'session_id', 'event_name', 'event_timestamp', 'batchID']])\n",
    "else:\n",
    "    print(\"\\nNo duplicates found by event_id + session_id + playerID.\")\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('event_session_player_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-dups-header",
   "metadata": {},
   "source": [
    "## 6. Content-Based Duplicates\n",
    "\n",
    "Identifying events with the same `playerID` + `event_name` + `event_timestamp` + `payload`. These are events that are identical in content regardless of their `event_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CONTENT-BASED DUPLICATES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create content key\n",
    "df['payload_str'] = df['payload'].fillna('').astype(str)\n",
    "df['content_key'] = df['playerID'] + '_' + df['event_name'] + '_' + df['event_timestamp'].astype(str) + '_' + df['payload_str']\n",
    "\n",
    "content_counts = df['content_key'].value_counts()\n",
    "duplicated_content = content_counts[content_counts > 1]\n",
    "\n",
    "unique_content = df['content_key'].nunique()\n",
    "duplicate_content_count = len(duplicated_content)\n",
    "events_with_dup_content = df[df['content_key'].isin(duplicated_content.index)]\n",
    "\n",
    "print(f\"\\nUnique content combinations: {unique_content:,}\")\n",
    "print(f\"Duplicated content combinations: {duplicate_content_count:,}\")\n",
    "print(f\"Events with duplicated content: {len(events_with_dup_content):,}\")\n",
    "print(f\"Duplication rate: {(len(df) - unique_content) / len(df) * 100:.2f}%\")\n",
    "\n",
    "if duplicate_content_count > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample content duplicates (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, count in duplicated_content.head(10).items():\n",
    "        print(f\"  {count} occurrences: {key[:80]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Sample duplicate events (first content key):\")\n",
    "    print(\"-\" * 40)\n",
    "    first_dup_key = duplicated_content.index[0]\n",
    "    display(df[df['content_key'] == first_dup_key][['event_id', 'playerID', 'event_name', 'event_timestamp', 'batchID']])\n",
    "else:\n",
    "    print(\"\\nNo content-based duplicates found.\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['payload_str', 'content_key'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "event-type-dups-header",
   "metadata": {},
   "source": [
    "## 7. Duplicate Trends by Event Type\n",
    "\n",
    "Analyzing which event types have the most duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "event-type-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE TRENDS BY EVENT TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicates by event_id\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "df['is_duplicate'] = df['event_id'].isin(duplicated_event_ids)\n",
    "\n",
    "# Analyze by event type\n",
    "event_type_analysis = df.groupby('event_name').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    duplicate_events=('is_duplicate', 'sum'),\n",
    "    unique_event_ids=('event_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "event_type_analysis['duplication_rate'] = (event_type_analysis['total_events'] - event_type_analysis['unique_event_ids']) / event_type_analysis['total_events'] * 100\n",
    "event_type_analysis = event_type_analysis.sort_values('duplicate_events', ascending=False)\n",
    "\n",
    "print(\"\\nDuplication by Event Type:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Event Type':<25} {'Total':>10} {'Duplicates':>12} {'Rate':>10}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in event_type_analysis.iterrows():\n",
    "    print(f\"{row['event_name']:<25} {row['total_events']:>10,} {int(row['duplicate_events']):>12,} {row['duplication_rate']:>9.2f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Duplicate count by event type\n",
    "ax1 = axes[0]\n",
    "event_type_analysis_sorted = event_type_analysis.sort_values('duplicate_events', ascending=True)\n",
    "ax1.barh(event_type_analysis_sorted['event_name'], event_type_analysis_sorted['duplicate_events'], color='coral', edgecolor='black')\n",
    "ax1.set_xlabel('Number of Duplicate Events')\n",
    "ax1.set_title('Duplicate Events by Type', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Duplication rate by event type\n",
    "ax2 = axes[1]\n",
    "event_type_analysis_rate = event_type_analysis.sort_values('duplication_rate', ascending=True)\n",
    "ax2.barh(event_type_analysis_rate['event_name'], event_type_analysis_rate['duplication_rate'], color='steelblue', edgecolor='black')\n",
    "ax2.set_xlabel('Duplication Rate (%)')\n",
    "ax2.set_title('Duplication Rate by Event Type', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "df.drop('is_duplicate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-dups-header",
   "metadata": {},
   "source": [
    "## 8. Duplicate Trends Over Time\n",
    "\n",
    "Analyzing how duplicates are distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE TRENDS OVER TIME\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicates\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "df['is_duplicate'] = df['event_id'].isin(duplicated_event_ids)\n",
    "\n",
    "# Add time-based columns\n",
    "df['hour'] = df['event_timestamp'].dt.floor('h')\n",
    "df['date'] = df['event_timestamp'].dt.date\n",
    "\n",
    "# Hourly analysis\n",
    "hourly_analysis = df.groupby('hour').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    duplicate_events=('is_duplicate', 'sum'),\n",
    "    unique_event_ids=('event_id', 'nunique')\n",
    ").reset_index()\n",
    "hourly_analysis['duplication_rate'] = (hourly_analysis['total_events'] - hourly_analysis['unique_event_ids']) / hourly_analysis['total_events'] * 100\n",
    "\n",
    "print(f\"\\nTime range: {df['event_timestamp'].min()} to {df['event_timestamp'].max()}\")\n",
    "print(f\"Total hours covered: {hourly_analysis['hour'].nunique()}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Events over time\n",
    "ax1 = axes[0]\n",
    "ax1.fill_between(hourly_analysis['hour'], hourly_analysis['total_events'], alpha=0.3, label='Total Events', color='steelblue')\n",
    "ax1.plot(hourly_analysis['hour'], hourly_analysis['total_events'], color='steelblue', linewidth=2, label='Total Events')\n",
    "ax1.fill_between(hourly_analysis['hour'], hourly_analysis['duplicate_events'], alpha=0.5, label='Duplicate Events', color='coral')\n",
    "ax1.plot(hourly_analysis['hour'], hourly_analysis['duplicate_events'], color='coral', linewidth=2, label='Duplicate Events')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Number of Events')\n",
    "ax1.set_title('Events Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Duplication rate over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(hourly_analysis['hour'], hourly_analysis['duplication_rate'], color='darkred', linewidth=2, marker='o', markersize=3)\n",
    "ax2.fill_between(hourly_analysis['hour'], hourly_analysis['duplication_rate'], alpha=0.3, color='darkred')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Duplication Rate (%)')\n",
    "ax2.set_title('Duplication Rate Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "df.drop(['is_duplicate', 'hour', 'date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-dups-header",
   "metadata": {},
   "source": [
    "## 9. Duplicates by Batch\n",
    "\n",
    "Analyzing whether duplicates occur within the same batch or across different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-dups",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES BY BATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicated event_ids\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "\n",
    "if len(duplicated_event_ids) > 0:\n",
    "    # Get duplicate events only\n",
    "    dup_events = df[df['event_id'].isin(duplicated_event_ids)]\n",
    "    \n",
    "    # Vectorized: count unique batches per event_id\n",
    "    batches_per_event = dup_events.groupby('event_id')['batchID'].nunique()\n",
    "    \n",
    "    within_batch = (batches_per_event == 1).sum()\n",
    "    cross_batch = (batches_per_event > 1).sum()\n",
    "    \n",
    "    print(f\"\\nTotal duplicated event_ids: {len(duplicated_event_ids):,}\")\n",
    "    print(f\"Within-batch duplicates: {within_batch:,} ({within_batch/len(duplicated_event_ids)*100:.1f}%)\")\n",
    "    print(f\"Cross-batch duplicates: {cross_batch:,} ({cross_batch/len(duplicated_event_ids)*100:.1f}%)\")\n",
    "    \n",
    "    # Batch-level analysis\n",
    "    batch_analysis = df.groupby('batchID').agg(\n",
    "        total_events=('event_id', 'count'),\n",
    "        unique_event_ids=('event_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    batch_analysis['duplicates_in_batch'] = batch_analysis['total_events'] - batch_analysis['unique_event_ids']\n",
    "    batch_analysis['duplication_rate'] = batch_analysis['duplicates_in_batch'] / batch_analysis['total_events'] * 100\n",
    "    batch_analysis = batch_analysis.sort_values('duplicates_in_batch', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Batches with most duplicates (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for _, row in batch_analysis.head(10).iterrows():\n",
    "        print(f\"  Batch {row['batchID'][:20]}...: {int(row['duplicates_in_batch']):,} duplicates ({row['duplication_rate']:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    labels = ['Within-Batch', 'Cross-Batch']\n",
    "    sizes = [within_batch, cross_batch]\n",
    "    colors = ['steelblue', 'coral']\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title('Distribution of Duplicate Types', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo duplicated event_ids found to analyze by batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "player-dups-analysis-header",
   "metadata": {},
   "source": [
    "## 10. Player-Level Analysis\n",
    "\n",
    "Identifying players with the most duplicate events and analyzing correlations with device/OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "player-dups-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PLAYER-LEVEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify duplicates\n",
    "event_id_counts = df['event_id'].value_counts()\n",
    "duplicated_event_ids = event_id_counts[event_id_counts > 1].index\n",
    "df['is_duplicate'] = df['event_id'].isin(duplicated_event_ids)\n",
    "\n",
    "# Player-level analysis\n",
    "player_analysis = df.groupby('playerID').agg(\n",
    "    total_events=('event_id', 'count'),\n",
    "    duplicate_events=('is_duplicate', 'sum'),\n",
    "    unique_event_ids=('event_id', 'nunique'),\n",
    "    device_os=('device_os', 'first'),\n",
    "    device_model=('device_model', 'first')\n",
    ").reset_index()\n",
    "\n",
    "player_analysis['duplication_rate'] = (player_analysis['total_events'] - player_analysis['unique_event_ids']) / player_analysis['total_events'] * 100\n",
    "player_analysis = player_analysis.sort_values('duplicate_events', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal players: {len(player_analysis):,}\")\n",
    "players_with_dups = player_analysis[player_analysis['duplicate_events'] > 0]\n",
    "print(f\"Players with duplicates: {len(players_with_dups):,} ({len(players_with_dups)/len(player_analysis)*100:.1f}%)\")\n",
    "\n",
    "if len(players_with_dups) > 0:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Players with most duplicates (top 10):\")\n",
    "    print(\"-\" * 40)\n",
    "    for _, row in player_analysis.head(10).iterrows():\n",
    "        print(f\"  {row['playerID']}: {int(row['duplicate_events']):,} duplicates ({row['duplication_rate']:.1f}%) - {row['device_os'][:30]}...\")\n",
    "    \n",
    "    # Analyze by device OS\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Duplicates by Device OS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Extract OS type (Android, iOS, Mac, etc.)\n",
    "    df['os_type'] = df['device_os'].apply(lambda x: 'Android' if 'Android' in str(x) else ('iOS' if 'iOS' in str(x) or 'iPhone' in str(x) else ('Mac' if 'Mac' in str(x) else 'Other')))\n",
    "    \n",
    "    os_analysis = df.groupby('os_type').agg(\n",
    "        total_events=('event_id', 'count'),\n",
    "        duplicate_events=('is_duplicate', 'sum'),\n",
    "        unique_event_ids=('event_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    os_analysis['duplication_rate'] = (os_analysis['total_events'] - os_analysis['unique_event_ids']) / os_analysis['total_events'] * 100\n",
    "    \n",
    "    for _, row in os_analysis.iterrows():\n",
    "        print(f\"  {row['os_type']}: {int(row['duplicate_events']):,} duplicates ({row['duplication_rate']:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Top players by duplicate count\n",
    "    ax1 = axes[0]\n",
    "    top_players = player_analysis.head(10)\n",
    "    ax1.barh(top_players['playerID'].str[:15], top_players['duplicate_events'], color='coral', edgecolor='black')\n",
    "    ax1.set_xlabel('Number of Duplicate Events')\n",
    "    ax1.set_title('Top 10 Players by Duplicate Count', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Duplicates by OS type\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(os_analysis['os_type'], os_analysis['duplicate_events'], color='steelblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Operating System')\n",
    "    ax2.set_ylabel('Number of Duplicate Events')\n",
    "    ax2.set_title('Duplicate Events by OS Type', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up\n",
    "    df.drop('os_type', axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"\\nNo player duplicates found.\")\n",
    "\n",
    "# Clean up\n",
    "df.drop('is_duplicate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "Overall summary of duplication findings across all detection strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate all metrics\n",
    "total_events = len(df)\n",
    "\n",
    "# Strategy 1: Exact duplicates by event_id\n",
    "unique_event_ids = df['event_id'].nunique()\n",
    "exact_dup_rate = (total_events - unique_event_ids) / total_events * 100\n",
    "\n",
    "# Strategy 2: Duplicates by event_id + playerID\n",
    "df['event_player_key'] = df['event_id'] + '_' + df['playerID']\n",
    "unique_event_player = df['event_player_key'].nunique()\n",
    "event_player_dup_rate = (total_events - unique_event_player) / total_events * 100\n",
    "df.drop('event_player_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 3: Duplicates by event_id + session_id\n",
    "df['event_session_key'] = df['event_id'] + '_' + df['session_id']\n",
    "unique_event_session = df['event_session_key'].nunique()\n",
    "event_session_dup_rate = (total_events - unique_event_session) / total_events * 100\n",
    "df.drop('event_session_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 4: Duplicates by event_id + session_id + playerID\n",
    "df['event_session_player_key'] = df['event_id'] + '_' + df['session_id'] + '_' + df['playerID']\n",
    "unique_event_session_player = df['event_session_player_key'].nunique()\n",
    "event_session_player_dup_rate = (total_events - unique_event_session_player) / total_events * 100\n",
    "df.drop('event_session_player_key', axis=1, inplace=True)\n",
    "\n",
    "# Strategy 5: Content-based duplicates\n",
    "df['payload_str'] = df['payload'].fillna('').astype(str)\n",
    "df['content_key'] = df['playerID'] + '_' + df['event_name'] + '_' + df['event_timestamp'].astype(str) + '_' + df['payload_str']\n",
    "unique_content = df['content_key'].nunique()\n",
    "content_dup_rate = (total_events - unique_content) / total_events * 100\n",
    "df.drop(['payload_str', 'content_key'], axis=1, inplace=True)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Detection Strategy': [\n",
    "        '1. Exact (by event_id)',\n",
    "        '2. event_id + playerID',\n",
    "        '3. event_id + session_id',\n",
    "        '4. event_id + session_id + playerID',\n",
    "        '5. Content-based (player+event+time+payload)'\n",
    "    ],\n",
    "    'Unique Count': [\n",
    "        unique_event_ids,\n",
    "        unique_event_player,\n",
    "        unique_event_session,\n",
    "        unique_event_session_player,\n",
    "        unique_content\n",
    "    ],\n",
    "    'Duplicate Count': [\n",
    "        total_events - unique_event_ids,\n",
    "        total_events - unique_event_player,\n",
    "        total_events - unique_event_session,\n",
    "        total_events - unique_event_session_player,\n",
    "        total_events - unique_content\n",
    "    ],\n",
    "    'Duplication Rate': [\n",
    "        f\"{exact_dup_rate:.2f}%\",\n",
    "        f\"{event_player_dup_rate:.2f}%\",\n",
    "        f\"{event_session_dup_rate:.2f}%\",\n",
    "        f\"{event_session_player_dup_rate:.2f}%\",\n",
    "        f\"{content_dup_rate:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"\\nTotal Events: {total_events:,}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Duplication Summary by Detection Strategy:\")\n",
    "print(\"-\" * 70)\n",
    "display(summary_df)\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if exact_dup_rate > 0:\n",
    "    print(f\"\\n- Found {exact_dup_rate:.2f}% exact duplicates by event_id\")\n",
    "else:\n",
    "    print(\"\\n- No exact duplicates found by event_id\")\n",
    "\n",
    "if content_dup_rate > exact_dup_rate:\n",
    "    print(f\"- Content-based analysis reveals {content_dup_rate:.2f}% duplicates (higher than exact ID check)\")\n",
    "    print(\"  This suggests some duplicate content has different event_ids\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "strategies = ['Exact\\n(event_id)', 'event_id +\\nplayerID', 'event_id +\\nsession_id', 'event_id +\\nsession_id +\\nplayerID', 'Content-\\nbased']\n",
    "rates = [exact_dup_rate, event_player_dup_rate, event_session_dup_rate, event_session_player_dup_rate, content_dup_rate]\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'orchid', 'gold']\n",
    "\n",
    "bars = ax.bar(strategies, rates, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Duplication Rate (%)')\n",
    "ax.set_title('Duplication Rate by Detection Strategy', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, rates):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{rate:.2f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
